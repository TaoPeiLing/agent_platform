[Skip to content](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/#openai-chat-completions-model)

# `OpenAI Chat Completions model`

### OpenAIChatCompletionsModel

Bases: `Model`

Source code in `src/agents/models/openai_chatcompletions.py`

|     |     |
| --- | --- |
| ```<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>440<br>441<br>442<br>443<br>444<br>445<br>446<br>447<br>448<br>449<br>450<br>451<br>452<br>453<br>454<br>455<br>456<br>457<br>458<br>459<br>460<br>461<br>462<br>463<br>464<br>465<br>466<br>467<br>468<br>469<br>470<br>471<br>472<br>473<br>474<br>475<br>476<br>477<br>478<br>479<br>480<br>481<br>482<br>483<br>484<br>485<br>486<br>487<br>488<br>489<br>490<br>491<br>492<br>493<br>494<br>495<br>496<br>497<br>498<br>499<br>500<br>501<br>502<br>503<br>504<br>505<br>506<br>507<br>508<br>509<br>510<br>511<br>512<br>513<br>514<br>515<br>516<br>517<br>518<br>519<br>520<br>521<br>522<br>523<br>524<br>525<br>526<br>527<br>528<br>529<br>530<br>531<br>532<br>533<br>534<br>535<br>536<br>537<br>538<br>539<br>540<br>541<br>542<br>543<br>544<br>545<br>546<br>547<br>548<br>549<br>550<br>551<br>552<br>553<br>554<br>555<br>``` | ```md-code__content<br>class OpenAIChatCompletionsModel(Model):<br>    def __init__(<br>        self,<br>        model: str | ChatModel,<br>        openai_client: AsyncOpenAI,<br>    ) -> None:<br>        self.model = model<br>        self._client = openai_client<br>    def _non_null_or_not_given(self, value: Any) -> Any:<br>        return value if value is not None else NOT_GIVEN<br>    async def get_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>    ) -> ModelResponse:<br>        with generation_span(<br>            model=str(self.model),<br>            model_config=dataclasses.asdict(model_settings)<br>            | {"base_url": str(self._client.base_url)},<br>            disabled=tracing.is_disabled(),<br>        ) as span_generation:<br>            response = await self._fetch_response(<br>                system_instructions,<br>                input,<br>                model_settings,<br>                tools,<br>                output_schema,<br>                handoffs,<br>                span_generation,<br>                tracing,<br>                stream=False,<br>            )<br>            if _debug.DONT_LOG_MODEL_DATA:<br>                logger.debug("Received model response")<br>            else:<br>                logger.debug(<br>                    f"LLM resp:\n{json.dumps(response.choices[0].message.model_dump(), indent=2)}\n"<br>                )<br>            usage = (<br>                Usage(<br>                    requests=1,<br>                    input_tokens=response.usage.prompt_tokens,<br>                    output_tokens=response.usage.completion_tokens,<br>                    total_tokens=response.usage.total_tokens,<br>                )<br>                if response.usage<br>                else Usage()<br>            )<br>            if tracing.include_data():<br>                span_generation.span_data.output = [response.choices[0].message.model_dump()]<br>            span_generation.span_data.usage = {<br>                "input_tokens": usage.input_tokens,<br>                "output_tokens": usage.output_tokens,<br>            }<br>            items = _Converter.message_to_output_items(response.choices[0].message)<br>            return ModelResponse(<br>                output=items,<br>                usage=usage,<br>                referenceable_id=None,<br>            )<br>    async def stream_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>    ) -> AsyncIterator[TResponseStreamEvent]:<br>        """<br>        Yields a partial message as it is generated, as well as the usage information.<br>        """<br>        with generation_span(<br>            model=str(self.model),<br>            model_config=dataclasses.asdict(model_settings)<br>            | {"base_url": str(self._client.base_url)},<br>            disabled=tracing.is_disabled(),<br>        ) as span_generation:<br>            response, stream = await self._fetch_response(<br>                system_instructions,<br>                input,<br>                model_settings,<br>                tools,<br>                output_schema,<br>                handoffs,<br>                span_generation,<br>                tracing,<br>                stream=True,<br>            )<br>            usage: CompletionUsage | None = None<br>            state = _StreamingState()<br>            async for chunk in stream:<br>                if not state.started:<br>                    state.started = True<br>                    yield ResponseCreatedEvent(<br>                        response=response,<br>                        type="response.created",<br>                    )<br>                # The usage is only available in the last chunk<br>                usage = chunk.usage<br>                if not chunk.choices or not chunk.choices[0].delta:<br>                    continue<br>                delta = chunk.choices[0].delta<br>                # Handle text<br>                if delta.content:<br>                    if not state.text_content_index_and_output:<br>                        # Initialize a content tracker for streaming text<br>                        state.text_content_index_and_output = (<br>                            0 if not state.refusal_content_index_and_output else 1,<br>                            ResponseOutputText(<br>                                text="",<br>                                type="output_text",<br>                                annotations=[],<br>                            ),<br>                        )<br>                        # Start a new assistant message stream<br>                        assistant_item = ResponseOutputMessage(<br>                            id=FAKE_RESPONSES_ID,<br>                            content=[],<br>                            role="assistant",<br>                            type="message",<br>                            status="in_progress",<br>                        )<br>                        # Notify consumers of the start of a new output message + first content part<br>                        yield ResponseOutputItemAddedEvent(<br>                            item=assistant_item,<br>                            output_index=0,<br>                            type="response.output_item.added",<br>                        )<br>                        yield ResponseContentPartAddedEvent(<br>                            content_index=state.text_content_index_and_output[0],<br>                            item_id=FAKE_RESPONSES_ID,<br>                            output_index=0,<br>                            part=ResponseOutputText(<br>                                text="",<br>                                type="output_text",<br>                                annotations=[],<br>                            ),<br>                            type="response.content_part.added",<br>                        )<br>                    # Emit the delta for this segment of content<br>                    yield ResponseTextDeltaEvent(<br>                        content_index=state.text_content_index_and_output[0],<br>                        delta=delta.content,<br>                        item_id=FAKE_RESPONSES_ID,<br>                        output_index=0,<br>                        type="response.output_text.delta",<br>                    )<br>                    # Accumulate the text into the response part<br>                    state.text_content_index_and_output[1].text += delta.content<br>                # Handle refusals (model declines to answer)<br>                if delta.refusal:<br>                    if not state.refusal_content_index_and_output:<br>                        # Initialize a content tracker for streaming refusal text<br>                        state.refusal_content_index_and_output = (<br>                            0 if not state.text_content_index_and_output else 1,<br>                            ResponseOutputRefusal(refusal="", type="refusal"),<br>                        )<br>                        # Start a new assistant message if one doesn't exist yet (in-progress)<br>                        assistant_item = ResponseOutputMessage(<br>                            id=FAKE_RESPONSES_ID,<br>                            content=[],<br>                            role="assistant",<br>                            type="message",<br>                            status="in_progress",<br>                        )<br>                        # Notify downstream that assistant message + first content part are starting<br>                        yield ResponseOutputItemAddedEvent(<br>                            item=assistant_item,<br>                            output_index=0,<br>                            type="response.output_item.added",<br>                        )<br>                        yield ResponseContentPartAddedEvent(<br>                            content_index=state.refusal_content_index_and_output[0],<br>                            item_id=FAKE_RESPONSES_ID,<br>                            output_index=0,<br>                            part=ResponseOutputText(<br>                                text="",<br>                                type="output_text",<br>                                annotations=[],<br>                            ),<br>                            type="response.content_part.added",<br>                        )<br>                    # Emit the delta for this segment of refusal<br>                    yield ResponseRefusalDeltaEvent(<br>                        content_index=state.refusal_content_index_and_output[0],<br>                        delta=delta.refusal,<br>                        item_id=FAKE_RESPONSES_ID,<br>                        output_index=0,<br>                        type="response.refusal.delta",<br>                    )<br>                    # Accumulate the refusal string in the output part<br>                    state.refusal_content_index_and_output[1].refusal += delta.refusal<br>                # Handle tool calls<br>                # Because we don't know the name of the function until the end of the stream, we'll<br>                # save everything and yield events at the end<br>                if delta.tool_calls:<br>                    for tc_delta in delta.tool_calls:<br>                        if tc_delta.index not in state.function_calls:<br>                            state.function_calls[tc_delta.index] = ResponseFunctionToolCall(<br>                                id=FAKE_RESPONSES_ID,<br>                                arguments="",<br>                                name="",<br>                                type="function_call",<br>                                call_id="",<br>                            )<br>                        tc_function = tc_delta.function<br>                        state.function_calls[tc_delta.index].arguments += (<br>                            tc_function.arguments if tc_function else ""<br>                        ) or ""<br>                        state.function_calls[tc_delta.index].name += (<br>                            tc_function.name if tc_function else ""<br>                        ) or ""<br>                        state.function_calls[tc_delta.index].call_id += tc_delta.id or ""<br>            function_call_starting_index = 0<br>            if state.text_content_index_and_output:<br>                function_call_starting_index += 1<br>                # Send end event for this content part<br>                yield ResponseContentPartDoneEvent(<br>                    content_index=state.text_content_index_and_output[0],<br>                    item_id=FAKE_RESPONSES_ID,<br>                    output_index=0,<br>                    part=state.text_content_index_and_output[1],<br>                    type="response.content_part.done",<br>                )<br>            if state.refusal_content_index_and_output:<br>                function_call_starting_index += 1<br>                # Send end event for this content part<br>                yield ResponseContentPartDoneEvent(<br>                    content_index=state.refusal_content_index_and_output[0],<br>                    item_id=FAKE_RESPONSES_ID,<br>                    output_index=0,<br>                    part=state.refusal_content_index_and_output[1],<br>                    type="response.content_part.done",<br>                )<br>            # Actually send events for the function calls<br>            for function_call in state.function_calls.values():<br>                # First, a ResponseOutputItemAdded for the function call<br>                yield ResponseOutputItemAddedEvent(<br>                    item=ResponseFunctionToolCall(<br>                        id=FAKE_RESPONSES_ID,<br>                        call_id=function_call.call_id,<br>                        arguments=function_call.arguments,<br>                        name=function_call.name,<br>                        type="function_call",<br>                    ),<br>                    output_index=function_call_starting_index,<br>                    type="response.output_item.added",<br>                )<br>                # Then, yield the args<br>                yield ResponseFunctionCallArgumentsDeltaEvent(<br>                    delta=function_call.arguments,<br>                    item_id=FAKE_RESPONSES_ID,<br>                    output_index=function_call_starting_index,<br>                    type="response.function_call_arguments.delta",<br>                )<br>                # Finally, the ResponseOutputItemDone<br>                yield ResponseOutputItemDoneEvent(<br>                    item=ResponseFunctionToolCall(<br>                        id=FAKE_RESPONSES_ID,<br>                        call_id=function_call.call_id,<br>                        arguments=function_call.arguments,<br>                        name=function_call.name,<br>                        type="function_call",<br>                    ),<br>                    output_index=function_call_starting_index,<br>                    type="response.output_item.done",<br>                )<br>            # Finally, send the Response completed event<br>            outputs: list[ResponseOutputItem] = []<br>            if state.text_content_index_and_output or state.refusal_content_index_and_output:<br>                assistant_msg = ResponseOutputMessage(<br>                    id=FAKE_RESPONSES_ID,<br>                    content=[],<br>                    role="assistant",<br>                    type="message",<br>                    status="completed",<br>                )<br>                if state.text_content_index_and_output:<br>                    assistant_msg.content.append(state.text_content_index_and_output[1])<br>                if state.refusal_content_index_and_output:<br>                    assistant_msg.content.append(state.refusal_content_index_and_output[1])<br>                outputs.append(assistant_msg)<br>                # send a ResponseOutputItemDone for the assistant message<br>                yield ResponseOutputItemDoneEvent(<br>                    item=assistant_msg,<br>                    output_index=0,<br>                    type="response.output_item.done",<br>                )<br>            for function_call in state.function_calls.values():<br>                outputs.append(function_call)<br>            final_response = response.model_copy()<br>            final_response.output = outputs<br>            final_response.usage = (<br>                ResponseUsage(<br>                    input_tokens=usage.prompt_tokens,<br>                    output_tokens=usage.completion_tokens,<br>                    total_tokens=usage.total_tokens,<br>                    output_tokens_details=OutputTokensDetails(<br>                        reasoning_tokens=usage.completion_tokens_details.reasoning_tokens<br>                        if usage.completion_tokens_details<br>                        and usage.completion_tokens_details.reasoning_tokens<br>                        else 0<br>                    ),<br>                )<br>                if usage<br>                else None<br>            )<br>            yield ResponseCompletedEvent(<br>                response=final_response,<br>                type="response.completed",<br>            )<br>            if tracing.include_data():<br>                span_generation.span_data.output = [final_response.model_dump()]<br>            if usage:<br>                span_generation.span_data.usage = {<br>                    "input_tokens": usage.prompt_tokens,<br>                    "output_tokens": usage.completion_tokens,<br>                }<br>    @overload<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: Literal[True],<br>    ) -> tuple[Response, AsyncStream[ChatCompletionChunk]]: ...<br>    @overload<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: Literal[False],<br>    ) -> ChatCompletion: ...<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchema | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: bool = False,<br>    ) -> ChatCompletion | tuple[Response, AsyncStream[ChatCompletionChunk]]:<br>        converted_messages = _Converter.items_to_messages(input)<br>        if system_instructions:<br>            converted_messages.insert(<br>                0,<br>                {<br>                    "content": system_instructions,<br>                    "role": "system",<br>                },<br>            )<br>        if tracing.include_data():<br>            span.span_data.input = converted_messages<br>        parallel_tool_calls = (<br>            True if model_settings.parallel_tool_calls and tools and len(tools) > 0 else NOT_GIVEN<br>        )<br>        tool_choice = _Converter.convert_tool_choice(model_settings.tool_choice)<br>        response_format = _Converter.convert_response_format(output_schema)<br>        converted_tools = [ToolConverter.to_openai(tool) for tool in tools] if tools else []<br>        for handoff in handoffs:<br>            converted_tools.append(ToolConverter.convert_handoff_tool(handoff))<br>        if _debug.DONT_LOG_MODEL_DATA:<br>            logger.debug("Calling LLM")<br>        else:<br>            logger.debug(<br>                f"{json.dumps(converted_messages, indent=2)}\n"<br>                f"Tools:\n{json.dumps(converted_tools, indent=2)}\n"<br>                f"Stream: {stream}\n"<br>                f"Tool choice: {tool_choice}\n"<br>                f"Response format: {response_format}\n"<br>            )<br>        ret = await self._get_client().chat.completions.create(<br>            model=self.model,<br>            messages=converted_messages,<br>            tools=converted_tools or NOT_GIVEN,<br>            temperature=self._non_null_or_not_given(model_settings.temperature),<br>            top_p=self._non_null_or_not_given(model_settings.top_p),<br>            frequency_penalty=self._non_null_or_not_given(model_settings.frequency_penalty),<br>            presence_penalty=self._non_null_or_not_given(model_settings.presence_penalty),<br>            max_tokens=self._non_null_or_not_given(model_settings.max_tokens),<br>            tool_choice=tool_choice,<br>            response_format=response_format,<br>            parallel_tool_calls=parallel_tool_calls,<br>            stream=stream,<br>            stream_options={"include_usage": True} if stream else NOT_GIVEN,<br>            extra_headers=_HEADERS,<br>        )<br>        if isinstance(ret, ChatCompletion):<br>            return ret<br>        response = Response(<br>            id=FAKE_RESPONSES_ID,<br>            created_at=time.time(),<br>            model=self.model,<br>            object="response",<br>            output=[],<br>            tool_choice=cast(Literal["auto", "required", "none"], tool_choice)<br>            if tool_choice != NOT_GIVEN<br>            else "auto",<br>            top_p=model_settings.top_p,<br>            temperature=model_settings.temperature,<br>            tools=[],<br>            parallel_tool_calls=parallel_tool_calls or False,<br>        )<br>        return response, ret<br>    def _get_client(self) -> AsyncOpenAI:<br>        if self._client is None:<br>            self._client = AsyncOpenAI()<br>        return self._client<br>``` |

#### stream\_response`async`

```md-code__content
stream_response(
    system_instructions: str | None,
    input: str | list[TResponseInputItem],
    model_settings: ModelSettings,
    tools: list[Tool],
    output_schema: AgentOutputSchema | None,
    handoffs: list[Handoff],
    tracing: ModelTracing,
) -> AsyncIterator[TResponseStreamEvent]

```

Yields a partial message as it is generated, as well as the usage information.

Source code in `src/agents/models/openai_chatcompletions.py`

|     |     |
| --- | --- |
| ```<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>342<br>343<br>344<br>345<br>346<br>347<br>348<br>349<br>350<br>351<br>352<br>353<br>354<br>355<br>356<br>357<br>358<br>359<br>360<br>361<br>362<br>363<br>364<br>365<br>366<br>367<br>368<br>369<br>370<br>371<br>372<br>373<br>374<br>375<br>376<br>377<br>378<br>379<br>380<br>381<br>382<br>383<br>384<br>385<br>386<br>387<br>388<br>389<br>390<br>391<br>392<br>393<br>394<br>395<br>396<br>397<br>398<br>399<br>400<br>401<br>402<br>403<br>404<br>405<br>406<br>407<br>408<br>409<br>410<br>411<br>412<br>413<br>414<br>415<br>416<br>417<br>418<br>419<br>420<br>421<br>422<br>423<br>424<br>425<br>426<br>427<br>428<br>429<br>430<br>431<br>432<br>433<br>434<br>435<br>436<br>437<br>438<br>439<br>``` | ```md-code__content<br>async def stream_response(<br>    self,<br>    system_instructions: str | None,<br>    input: str | list[TResponseInputItem],<br>    model_settings: ModelSettings,<br>    tools: list[Tool],<br>    output_schema: AgentOutputSchema | None,<br>    handoffs: list[Handoff],<br>    tracing: ModelTracing,<br>) -> AsyncIterator[TResponseStreamEvent]:<br>    """<br>    Yields a partial message as it is generated, as well as the usage information.<br>    """<br>    with generation_span(<br>        model=str(self.model),<br>        model_config=dataclasses.asdict(model_settings)<br>        | {"base_url": str(self._client.base_url)},<br>        disabled=tracing.is_disabled(),<br>    ) as span_generation:<br>        response, stream = await self._fetch_response(<br>            system_instructions,<br>            input,<br>            model_settings,<br>            tools,<br>            output_schema,<br>            handoffs,<br>            span_generation,<br>            tracing,<br>            stream=True,<br>        )<br>        usage: CompletionUsage | None = None<br>        state = _StreamingState()<br>        async for chunk in stream:<br>            if not state.started:<br>                state.started = True<br>                yield ResponseCreatedEvent(<br>                    response=response,<br>                    type="response.created",<br>                )<br>            # The usage is only available in the last chunk<br>            usage = chunk.usage<br>            if not chunk.choices or not chunk.choices[0].delta:<br>                continue<br>            delta = chunk.choices[0].delta<br>            # Handle text<br>            if delta.content:<br>                if not state.text_content_index_and_output:<br>                    # Initialize a content tracker for streaming text<br>                    state.text_content_index_and_output = (<br>                        0 if not state.refusal_content_index_and_output else 1,<br>                        ResponseOutputText(<br>                            text="",<br>                            type="output_text",<br>                            annotations=[],<br>                        ),<br>                    )<br>                    # Start a new assistant message stream<br>                    assistant_item = ResponseOutputMessage(<br>                        id=FAKE_RESPONSES_ID,<br>                        content=[],<br>                        role="assistant",<br>                        type="message",<br>                        status="in_progress",<br>                    )<br>                    # Notify consumers of the start of a new output message + first content part<br>                    yield ResponseOutputItemAddedEvent(<br>                        item=assistant_item,<br>                        output_index=0,<br>                        type="response.output_item.added",<br>                    )<br>                    yield ResponseContentPartAddedEvent(<br>                        content_index=state.text_content_index_and_output[0],<br>                        item_id=FAKE_RESPONSES_ID,<br>                        output_index=0,<br>                        part=ResponseOutputText(<br>                            text="",<br>                            type="output_text",<br>                            annotations=[],<br>                        ),<br>                        type="response.content_part.added",<br>                    )<br>                # Emit the delta for this segment of content<br>                yield ResponseTextDeltaEvent(<br>                    content_index=state.text_content_index_and_output[0],<br>                    delta=delta.content,<br>                    item_id=FAKE_RESPONSES_ID,<br>                    output_index=0,<br>                    type="response.output_text.delta",<br>                )<br>                # Accumulate the text into the response part<br>                state.text_content_index_and_output[1].text += delta.content<br>            # Handle refusals (model declines to answer)<br>            if delta.refusal:<br>                if not state.refusal_content_index_and_output:<br>                    # Initialize a content tracker for streaming refusal text<br>                    state.refusal_content_index_and_output = (<br>                        0 if not state.text_content_index_and_output else 1,<br>                        ResponseOutputRefusal(refusal="", type="refusal"),<br>                    )<br>                    # Start a new assistant message if one doesn't exist yet (in-progress)<br>                    assistant_item = ResponseOutputMessage(<br>                        id=FAKE_RESPONSES_ID,<br>                        content=[],<br>                        role="assistant",<br>                        type="message",<br>                        status="in_progress",<br>                    )<br>                    # Notify downstream that assistant message + first content part are starting<br>                    yield ResponseOutputItemAddedEvent(<br>                        item=assistant_item,<br>                        output_index=0,<br>                        type="response.output_item.added",<br>                    )<br>                    yield ResponseContentPartAddedEvent(<br>                        content_index=state.refusal_content_index_and_output[0],<br>                        item_id=FAKE_RESPONSES_ID,<br>                        output_index=0,<br>                        part=ResponseOutputText(<br>                            text="",<br>                            type="output_text",<br>                            annotations=[],<br>                        ),<br>                        type="response.content_part.added",<br>                    )<br>                # Emit the delta for this segment of refusal<br>                yield ResponseRefusalDeltaEvent(<br>                    content_index=state.refusal_content_index_and_output[0],<br>                    delta=delta.refusal,<br>                    item_id=FAKE_RESPONSES_ID,<br>                    output_index=0,<br>                    type="response.refusal.delta",<br>                )<br>                # Accumulate the refusal string in the output part<br>                state.refusal_content_index_and_output[1].refusal += delta.refusal<br>            # Handle tool calls<br>            # Because we don't know the name of the function until the end of the stream, we'll<br>            # save everything and yield events at the end<br>            if delta.tool_calls:<br>                for tc_delta in delta.tool_calls:<br>                    if tc_delta.index not in state.function_calls:<br>                        state.function_calls[tc_delta.index] = ResponseFunctionToolCall(<br>                            id=FAKE_RESPONSES_ID,<br>                            arguments="",<br>                            name="",<br>                            type="function_call",<br>                            call_id="",<br>                        )<br>                    tc_function = tc_delta.function<br>                    state.function_calls[tc_delta.index].arguments += (<br>                        tc_function.arguments if tc_function else ""<br>                    ) or ""<br>                    state.function_calls[tc_delta.index].name += (<br>                        tc_function.name if tc_function else ""<br>                    ) or ""<br>                    state.function_calls[tc_delta.index].call_id += tc_delta.id or ""<br>        function_call_starting_index = 0<br>        if state.text_content_index_and_output:<br>            function_call_starting_index += 1<br>            # Send end event for this content part<br>            yield ResponseContentPartDoneEvent(<br>                content_index=state.text_content_index_and_output[0],<br>                item_id=FAKE_RESPONSES_ID,<br>                output_index=0,<br>                part=state.text_content_index_and_output[1],<br>                type="response.content_part.done",<br>            )<br>        if state.refusal_content_index_and_output:<br>            function_call_starting_index += 1<br>            # Send end event for this content part<br>            yield ResponseContentPartDoneEvent(<br>                content_index=state.refusal_content_index_and_output[0],<br>                item_id=FAKE_RESPONSES_ID,<br>                output_index=0,<br>                part=state.refusal_content_index_and_output[1],<br>                type="response.content_part.done",<br>            )<br>        # Actually send events for the function calls<br>        for function_call in state.function_calls.values():<br>            # First, a ResponseOutputItemAdded for the function call<br>            yield ResponseOutputItemAddedEvent(<br>                item=ResponseFunctionToolCall(<br>                    id=FAKE_RESPONSES_ID,<br>                    call_id=function_call.call_id,<br>                    arguments=function_call.arguments,<br>                    name=function_call.name,<br>                    type="function_call",<br>                ),<br>                output_index=function_call_starting_index,<br>                type="response.output_item.added",<br>            )<br>            # Then, yield the args<br>            yield ResponseFunctionCallArgumentsDeltaEvent(<br>                delta=function_call.arguments,<br>                item_id=FAKE_RESPONSES_ID,<br>                output_index=function_call_starting_index,<br>                type="response.function_call_arguments.delta",<br>            )<br>            # Finally, the ResponseOutputItemDone<br>            yield ResponseOutputItemDoneEvent(<br>                item=ResponseFunctionToolCall(<br>                    id=FAKE_RESPONSES_ID,<br>                    call_id=function_call.call_id,<br>                    arguments=function_call.arguments,<br>                    name=function_call.name,<br>                    type="function_call",<br>                ),<br>                output_index=function_call_starting_index,<br>                type="response.output_item.done",<br>            )<br>        # Finally, send the Response completed event<br>        outputs: list[ResponseOutputItem] = []<br>        if state.text_content_index_and_output or state.refusal_content_index_and_output:<br>            assistant_msg = ResponseOutputMessage(<br>                id=FAKE_RESPONSES_ID,<br>                content=[],<br>                role="assistant",<br>                type="message",<br>                status="completed",<br>            )<br>            if state.text_content_index_and_output:<br>                assistant_msg.content.append(state.text_content_index_and_output[1])<br>            if state.refusal_content_index_and_output:<br>                assistant_msg.content.append(state.refusal_content_index_and_output[1])<br>            outputs.append(assistant_msg)<br>            # send a ResponseOutputItemDone for the assistant message<br>            yield ResponseOutputItemDoneEvent(<br>                item=assistant_msg,<br>                output_index=0,<br>                type="response.output_item.done",<br>            )<br>        for function_call in state.function_calls.values():<br>            outputs.append(function_call)<br>        final_response = response.model_copy()<br>        final_response.output = outputs<br>        final_response.usage = (<br>            ResponseUsage(<br>                input_tokens=usage.prompt_tokens,<br>                output_tokens=usage.completion_tokens,<br>                total_tokens=usage.total_tokens,<br>                output_tokens_details=OutputTokensDetails(<br>                    reasoning_tokens=usage.completion_tokens_details.reasoning_tokens<br>                    if usage.completion_tokens_details<br>                    and usage.completion_tokens_details.reasoning_tokens<br>                    else 0<br>                ),<br>            )<br>            if usage<br>            else None<br>        )<br>        yield ResponseCompletedEvent(<br>            response=final_response,<br>            type="response.completed",<br>        )<br>        if tracing.include_data():<br>            span_generation.span_data.output = [final_response.model_dump()]<br>        if usage:<br>            span_generation.span_data.usage = {<br>                "input_tokens": usage.prompt_tokens,<br>                "output_tokens": usage.completion_tokens,<br>            }<br>``` |

### \_Converter

Source code in `src/agents/models/openai_chatcompletions.py`

|     |     |
| --- | --- |
| ```<br>558<br>559<br>560<br>561<br>562<br>563<br>564<br>565<br>566<br>567<br>568<br>569<br>570<br>571<br>572<br>573<br>574<br>575<br>576<br>577<br>578<br>579<br>580<br>581<br>582<br>583<br>584<br>585<br>586<br>587<br>588<br>589<br>590<br>591<br>592<br>593<br>594<br>595<br>596<br>597<br>598<br>599<br>600<br>601<br>602<br>603<br>604<br>605<br>606<br>607<br>608<br>609<br>610<br>611<br>612<br>613<br>614<br>615<br>616<br>617<br>618<br>619<br>620<br>621<br>622<br>623<br>624<br>625<br>626<br>627<br>628<br>629<br>630<br>631<br>632<br>633<br>634<br>635<br>636<br>637<br>638<br>639<br>640<br>641<br>642<br>643<br>644<br>645<br>646<br>647<br>648<br>649<br>650<br>651<br>652<br>653<br>654<br>655<br>656<br>657<br>658<br>659<br>660<br>661<br>662<br>663<br>664<br>665<br>666<br>667<br>668<br>669<br>670<br>671<br>672<br>673<br>674<br>675<br>676<br>677<br>678<br>679<br>680<br>681<br>682<br>683<br>684<br>685<br>686<br>687<br>688<br>689<br>690<br>691<br>692<br>693<br>694<br>695<br>696<br>697<br>698<br>699<br>700<br>701<br>702<br>703<br>704<br>705<br>706<br>707<br>708<br>709<br>710<br>711<br>712<br>713<br>714<br>715<br>716<br>717<br>718<br>719<br>720<br>721<br>722<br>723<br>724<br>725<br>726<br>727<br>728<br>729<br>730<br>731<br>732<br>733<br>734<br>735<br>736<br>737<br>738<br>739<br>740<br>741<br>742<br>743<br>744<br>745<br>746<br>747<br>748<br>749<br>750<br>751<br>752<br>753<br>754<br>755<br>756<br>757<br>758<br>759<br>760<br>761<br>762<br>763<br>764<br>765<br>766<br>767<br>768<br>769<br>770<br>771<br>772<br>773<br>774<br>775<br>776<br>777<br>778<br>779<br>780<br>781<br>782<br>783<br>784<br>785<br>786<br>787<br>788<br>789<br>790<br>791<br>792<br>793<br>794<br>795<br>796<br>797<br>798<br>799<br>800<br>801<br>802<br>803<br>804<br>805<br>806<br>807<br>808<br>809<br>810<br>811<br>812<br>813<br>814<br>815<br>816<br>817<br>818<br>819<br>820<br>821<br>822<br>823<br>824<br>825<br>826<br>827<br>828<br>829<br>830<br>831<br>832<br>833<br>834<br>835<br>836<br>837<br>838<br>839<br>840<br>841<br>842<br>843<br>844<br>845<br>846<br>847<br>848<br>849<br>850<br>851<br>852<br>853<br>854<br>855<br>856<br>857<br>858<br>859<br>860<br>861<br>862<br>863<br>864<br>865<br>866<br>867<br>868<br>869<br>870<br>871<br>872<br>873<br>874<br>875<br>876<br>877<br>878<br>879<br>880<br>881<br>882<br>883<br>884<br>885<br>886<br>887<br>888<br>889<br>890<br>891<br>892<br>893<br>894<br>895<br>896<br>897<br>898<br>899<br>900<br>901<br>902<br>903<br>904<br>905<br>906<br>907<br>908<br>909<br>910<br>911<br>912<br>913<br>914<br>915<br>916<br>917<br>918<br>919<br>920<br>921<br>922<br>923<br>924<br>925<br>926<br>927<br>928<br>929<br>930<br>931<br>932<br>933<br>934<br>935<br>936<br>937<br>938<br>939<br>940<br>941<br>942<br>943<br>944<br>945<br>946<br>947<br>948<br>``` | ```md-code__content<br>class _Converter:<br>    @classmethod<br>    def convert_tool_choice(<br>        cls, tool_choice: Literal["auto", "required", "none"] | str | None<br>    ) -> ChatCompletionToolChoiceOptionParam | NotGiven:<br>        if tool_choice is None:<br>            return NOT_GIVEN<br>        elif tool_choice == "auto":<br>            return "auto"<br>        elif tool_choice == "required":<br>            return "required"<br>        elif tool_choice == "none":<br>            return "none"<br>        else:<br>            return {<br>                "type": "function",<br>                "function": {<br>                    "name": tool_choice,<br>                },<br>            }<br>    @classmethod<br>    def convert_response_format(<br>        cls, final_output_schema: AgentOutputSchema | None<br>    ) -> ResponseFormat | NotGiven:<br>        if not final_output_schema or final_output_schema.is_plain_text():<br>            return NOT_GIVEN<br>        return {<br>            "type": "json_schema",<br>            "json_schema": {<br>                "name": "final_output",<br>                "strict": final_output_schema.strict_json_schema,<br>                "schema": final_output_schema.json_schema(),<br>            },<br>        }<br>    @classmethod<br>    def message_to_output_items(cls, message: ChatCompletionMessage) -> list[TResponseOutputItem]:<br>        items: list[TResponseOutputItem] = []<br>        message_item = ResponseOutputMessage(<br>            id=FAKE_RESPONSES_ID,<br>            content=[],<br>            role="assistant",<br>            type="message",<br>            status="completed",<br>        )<br>        if message.content:<br>            message_item.content.append(<br>                ResponseOutputText(text=message.content, type="output_text", annotations=[])<br>            )<br>        if message.refusal:<br>            message_item.content.append(<br>                ResponseOutputRefusal(refusal=message.refusal, type="refusal")<br>            )<br>        if message.audio:<br>            raise AgentsException("Audio is not currently supported")<br>        if message_item.content:<br>            items.append(message_item)<br>        if message.tool_calls:<br>            for tool_call in message.tool_calls:<br>                items.append(<br>                    ResponseFunctionToolCall(<br>                        id=FAKE_RESPONSES_ID,<br>                        call_id=tool_call.id,<br>                        arguments=tool_call.function.arguments,<br>                        name=tool_call.function.name,<br>                        type="function_call",<br>                    )<br>                )<br>        return items<br>    @classmethod<br>    def maybe_easy_input_message(cls, item: Any) -> EasyInputMessageParam | None:<br>        if not isinstance(item, dict):<br>            return None<br>        keys = item.keys()<br>        # EasyInputMessageParam only has these two keys<br>        if keys != {"content", "role"}:<br>            return None<br>        role = item.get("role", None)<br>        if role not in ("user", "assistant", "system", "developer"):<br>            return None<br>        if "content" not in item:<br>            return None<br>        return cast(EasyInputMessageParam, item)<br>    @classmethod<br>    def maybe_input_message(cls, item: Any) -> Message | None:<br>        if (<br>            isinstance(item, dict)<br>            and item.get("type") == "message"<br>            and item.get("role")<br>            in (<br>                "user",<br>                "system",<br>                "developer",<br>            )<br>        ):<br>            return cast(Message, item)<br>        return None<br>    @classmethod<br>    def maybe_file_search_call(cls, item: Any) -> ResponseFileSearchToolCallParam | None:<br>        if isinstance(item, dict) and item.get("type") == "file_search_call":<br>            return cast(ResponseFileSearchToolCallParam, item)<br>        return None<br>    @classmethod<br>    def maybe_function_tool_call(cls, item: Any) -> ResponseFunctionToolCallParam | None:<br>        if isinstance(item, dict) and item.get("type") == "function_call":<br>            return cast(ResponseFunctionToolCallParam, item)<br>        return None<br>    @classmethod<br>    def maybe_function_tool_call_output(<br>        cls,<br>        item: Any,<br>    ) -> FunctionCallOutput | None:<br>        if isinstance(item, dict) and item.get("type") == "function_call_output":<br>            return cast(FunctionCallOutput, item)<br>        return None<br>    @classmethod<br>    def maybe_item_reference(cls, item: Any) -> ItemReference | None:<br>        if isinstance(item, dict) and item.get("type") == "item_reference":<br>            return cast(ItemReference, item)<br>        return None<br>    @classmethod<br>    def maybe_response_output_message(cls, item: Any) -> ResponseOutputMessageParam | None:<br>        # ResponseOutputMessage is only used for messages with role assistant<br>        if (<br>            isinstance(item, dict)<br>            and item.get("type") == "message"<br>            and item.get("role") == "assistant"<br>        ):<br>            return cast(ResponseOutputMessageParam, item)<br>        return None<br>    @classmethod<br>    def extract_text_content(<br>        cls, content: str | Iterable[ResponseInputContentParam]<br>    ) -> str | list[ChatCompletionContentPartTextParam]:<br>        all_content = cls.extract_all_content(content)<br>        if isinstance(all_content, str):<br>            return all_content<br>        out: list[ChatCompletionContentPartTextParam] = []<br>        for c in all_content:<br>            if c.get("type") == "text":<br>                out.append(cast(ChatCompletionContentPartTextParam, c))<br>        return out<br>    @classmethod<br>    def extract_all_content(<br>        cls, content: str | Iterable[ResponseInputContentParam]<br>    ) -> str | list[ChatCompletionContentPartParam]:<br>        if isinstance(content, str):<br>            return content<br>        out: list[ChatCompletionContentPartParam] = []<br>        for c in content:<br>            if isinstance(c, dict) and c.get("type") == "input_text":<br>                casted_text_param = cast(ResponseInputTextParam, c)<br>                out.append(<br>                    ChatCompletionContentPartTextParam(<br>                        type="text",<br>                        text=casted_text_param["text"],<br>                    )<br>                )<br>            elif isinstance(c, dict) and c.get("type") == "input_image":<br>                casted_image_param = cast(ResponseInputImageParam, c)<br>                if "image_url" not in casted_image_param or not casted_image_param["image_url"]:<br>                    raise UserError(<br>                        f"Only image URLs are supported for input_image {casted_image_param}"<br>                    )<br>                out.append(<br>                    ChatCompletionContentPartImageParam(<br>                        type="image_url",<br>                        image_url={<br>                            "url": casted_image_param["image_url"],<br>                            "detail": casted_image_param["detail"],<br>                        },<br>                    )<br>                )<br>            elif isinstance(c, dict) and c.get("type") == "input_file":<br>                raise UserError(f"File uploads are not supported for chat completions {c}")<br>            else:<br>                raise UserError(f"Unknonw content: {c}")<br>        return out<br>    @classmethod<br>    def items_to_messages(<br>        cls,<br>        items: str | Iterable[TResponseInputItem],<br>    ) -> list[ChatCompletionMessageParam]:<br>        """<br>        Convert a sequence of 'Item' objects into a list of ChatCompletionMessageParam.<br>        Rules:<br>        - EasyInputMessage or InputMessage (role=user) => ChatCompletionUserMessageParam<br>        - EasyInputMessage or InputMessage (role=system) => ChatCompletionSystemMessageParam<br>        - EasyInputMessage or InputMessage (role=developer) => ChatCompletionDeveloperMessageParam<br>        - InputMessage (role=assistant) => Start or flush a ChatCompletionAssistantMessageParam<br>        - response_output_message => Also produces/flushes a ChatCompletionAssistantMessageParam<br>        - tool calls get attached to the *current* assistant message, or create one if none.<br>        - tool outputs => ChatCompletionToolMessageParam<br>        """<br>        if isinstance(items, str):<br>            return [<br>                ChatCompletionUserMessageParam(<br>                    role="user",<br>                    content=items,<br>                )<br>            ]<br>        result: list[ChatCompletionMessageParam] = []<br>        current_assistant_msg: ChatCompletionAssistantMessageParam | None = None<br>        def flush_assistant_message() -> None:<br>            nonlocal current_assistant_msg<br>            if current_assistant_msg is not None:<br>                # The API doesn't support empty arrays for tool_calls<br>                if not current_assistant_msg.get("tool_calls"):<br>                    del current_assistant_msg["tool_calls"]<br>                result.append(current_assistant_msg)<br>                current_assistant_msg = None<br>        def ensure_assistant_message() -> ChatCompletionAssistantMessageParam:<br>            nonlocal current_assistant_msg<br>            if current_assistant_msg is None:<br>                current_assistant_msg = ChatCompletionAssistantMessageParam(role="assistant")<br>                current_assistant_msg["tool_calls"] = []<br>            return current_assistant_msg<br>        for item in items:<br>            # 1) Check easy input message<br>            if easy_msg := cls.maybe_easy_input_message(item):<br>                role = easy_msg["role"]<br>                content = easy_msg["content"]<br>                if role == "user":<br>                    flush_assistant_message()<br>                    msg_user: ChatCompletionUserMessageParam = {<br>                        "role": "user",<br>                        "content": cls.extract_all_content(content),<br>                    }<br>                    result.append(msg_user)<br>                elif role == "system":<br>                    flush_assistant_message()<br>                    msg_system: ChatCompletionSystemMessageParam = {<br>                        "role": "system",<br>                        "content": cls.extract_text_content(content),<br>                    }<br>                    result.append(msg_system)<br>                elif role == "developer":<br>                    flush_assistant_message()<br>                    msg_developer: ChatCompletionDeveloperMessageParam = {<br>                        "role": "developer",<br>                        "content": cls.extract_text_content(content),<br>                    }<br>                    result.append(msg_developer)<br>                elif role == "assistant":<br>                    flush_assistant_message()<br>                    msg_assistant: ChatCompletionAssistantMessageParam = {<br>                        "role": "assistant",<br>                        "content": cls.extract_text_content(content),<br>                    }<br>                    result.append(msg_assistant)<br>                else:<br>                    raise UserError(f"Unexpected role in easy_input_message: {role}")<br>            # 2) Check input message<br>            elif in_msg := cls.maybe_input_message(item):<br>                role = in_msg["role"]<br>                content = in_msg["content"]<br>                flush_assistant_message()<br>                if role == "user":<br>                    msg_user = {<br>                        "role": "user",<br>                        "content": cls.extract_all_content(content),<br>                    }<br>                    result.append(msg_user)<br>                elif role == "system":<br>                    msg_system = {<br>                        "role": "system",<br>                        "content": cls.extract_text_content(content),<br>                    }<br>                    result.append(msg_system)<br>                elif role == "developer":<br>                    msg_developer = {<br>                        "role": "developer",<br>                        "content": cls.extract_text_content(content),<br>                    }<br>                    result.append(msg_developer)<br>                else:<br>                    raise UserError(f"Unexpected role in input_message: {role}")<br>            # 3) response output message => assistant<br>            elif resp_msg := cls.maybe_response_output_message(item):<br>                flush_assistant_message()<br>                new_asst = ChatCompletionAssistantMessageParam(role="assistant")<br>                contents = resp_msg["content"]<br>                text_segments = []<br>                for c in contents:<br>                    if c["type"] == "output_text":<br>                        text_segments.append(c["text"])<br>                    elif c["type"] == "refusal":<br>                        new_asst["refusal"] = c["refusal"]<br>                    elif c["type"] == "output_audio":<br>                        # Can't handle this, b/c chat completions expects an ID which we dont have<br>                        raise UserError(<br>                            f"Only audio IDs are supported for chat completions, but got: {c}"<br>                        )<br>                    else:<br>                        raise UserError(f"Unknown content type in ResponseOutputMessage: {c}")<br>                if text_segments:<br>                    combined = "\n".join(text_segments)<br>                    new_asst["content"] = combined<br>                new_asst["tool_calls"] = []<br>                current_assistant_msg = new_asst<br>            # 4) function/file-search calls => attach to assistant<br>            elif file_search := cls.maybe_file_search_call(item):<br>                asst = ensure_assistant_message()<br>                tool_calls = list(asst.get("tool_calls", []))<br>                new_tool_call = ChatCompletionMessageToolCallParam(<br>                    id=file_search["id"],<br>                    type="function",<br>                    function={<br>                        "name": "file_search_call",<br>                        "arguments": json.dumps(<br>                            {<br>                                "queries": file_search.get("queries", []),<br>                                "status": file_search.get("status"),<br>                            }<br>                        ),<br>                    },<br>                )<br>                tool_calls.append(new_tool_call)<br>                asst["tool_calls"] = tool_calls<br>            elif func_call := cls.maybe_function_tool_call(item):<br>                asst = ensure_assistant_message()<br>                tool_calls = list(asst.get("tool_calls", []))<br>                new_tool_call = ChatCompletionMessageToolCallParam(<br>                    id=func_call["call_id"],<br>                    type="function",<br>                    function={<br>                        "name": func_call["name"],<br>                        "arguments": func_call["arguments"],<br>                    },<br>                )<br>                tool_calls.append(new_tool_call)<br>                asst["tool_calls"] = tool_calls<br>            # 5) function call output => tool message<br>            elif func_output := cls.maybe_function_tool_call_output(item):<br>                flush_assistant_message()<br>                msg: ChatCompletionToolMessageParam = {<br>                    "role": "tool",<br>                    "tool_call_id": func_output["call_id"],<br>                    "content": func_output["output"],<br>                }<br>                result.append(msg)<br>            # 6) item reference => handle or raise<br>            elif item_ref := cls.maybe_item_reference(item):<br>                raise UserError(<br>                    f"Encountered an item_reference, which is not supported: {item_ref}"<br>                )<br>            # 7) If we haven't recognized it => fail or ignore<br>            else:<br>                raise UserError(f"Unhandled item type or structure: {item}")<br>        flush_assistant_message()<br>        return result<br>``` |

#### items\_to\_messages`classmethod`

```md-code__content
items_to_messages(
    items: str | Iterable[TResponseInputItem],
) -> list[ChatCompletionMessageParam]

```

Convert a sequence of 'Item' objects into a list of ChatCompletionMessageParam.

Rules:
\- EasyInputMessage or InputMessage (role=user) => ChatCompletionUserMessageParam
\- EasyInputMessage or InputMessage (role=system) => ChatCompletionSystemMessageParam
\- EasyInputMessage or InputMessage (role=developer) => ChatCompletionDeveloperMessageParam
\- InputMessage (role=assistant) => Start or flush a ChatCompletionAssistantMessageParam
\- response\_output\_message => Also produces/flushes a ChatCompletionAssistantMessageParam
\- tool calls get attached to the _current_ assistant message, or create one if none.
\- tool outputs => ChatCompletionToolMessageParam

Source code in `src/agents/models/openai_chatcompletions.py`

|     |     |
| --- | --- |
| ```<br>758<br>759<br>760<br>761<br>762<br>763<br>764<br>765<br>766<br>767<br>768<br>769<br>770<br>771<br>772<br>773<br>774<br>775<br>776<br>777<br>778<br>779<br>780<br>781<br>782<br>783<br>784<br>785<br>786<br>787<br>788<br>789<br>790<br>791<br>792<br>793<br>794<br>795<br>796<br>797<br>798<br>799<br>800<br>801<br>802<br>803<br>804<br>805<br>806<br>807<br>808<br>809<br>810<br>811<br>812<br>813<br>814<br>815<br>816<br>817<br>818<br>819<br>820<br>821<br>822<br>823<br>824<br>825<br>826<br>827<br>828<br>829<br>830<br>831<br>832<br>833<br>834<br>835<br>836<br>837<br>838<br>839<br>840<br>841<br>842<br>843<br>844<br>845<br>846<br>847<br>848<br>849<br>850<br>851<br>852<br>853<br>854<br>855<br>856<br>857<br>858<br>859<br>860<br>861<br>862<br>863<br>864<br>865<br>866<br>867<br>868<br>869<br>870<br>871<br>872<br>873<br>874<br>875<br>876<br>877<br>878<br>879<br>880<br>881<br>882<br>883<br>884<br>885<br>886<br>887<br>888<br>889<br>890<br>891<br>892<br>893<br>894<br>895<br>896<br>897<br>898<br>899<br>900<br>901<br>902<br>903<br>904<br>905<br>906<br>907<br>908<br>909<br>910<br>911<br>912<br>913<br>914<br>915<br>916<br>917<br>918<br>919<br>920<br>921<br>922<br>923<br>924<br>925<br>926<br>927<br>928<br>929<br>930<br>931<br>932<br>933<br>934<br>935<br>936<br>937<br>938<br>939<br>940<br>941<br>942<br>943<br>944<br>945<br>946<br>947<br>948<br>``` | ```md-code__content<br>@classmethod<br>def items_to_messages(<br>    cls,<br>    items: str | Iterable[TResponseInputItem],<br>) -> list[ChatCompletionMessageParam]:<br>    """<br>    Convert a sequence of 'Item' objects into a list of ChatCompletionMessageParam.<br>    Rules:<br>    - EasyInputMessage or InputMessage (role=user) => ChatCompletionUserMessageParam<br>    - EasyInputMessage or InputMessage (role=system) => ChatCompletionSystemMessageParam<br>    - EasyInputMessage or InputMessage (role=developer) => ChatCompletionDeveloperMessageParam<br>    - InputMessage (role=assistant) => Start or flush a ChatCompletionAssistantMessageParam<br>    - response_output_message => Also produces/flushes a ChatCompletionAssistantMessageParam<br>    - tool calls get attached to the *current* assistant message, or create one if none.<br>    - tool outputs => ChatCompletionToolMessageParam<br>    """<br>    if isinstance(items, str):<br>        return [<br>            ChatCompletionUserMessageParam(<br>                role="user",<br>                content=items,<br>            )<br>        ]<br>    result: list[ChatCompletionMessageParam] = []<br>    current_assistant_msg: ChatCompletionAssistantMessageParam | None = None<br>    def flush_assistant_message() -> None:<br>        nonlocal current_assistant_msg<br>        if current_assistant_msg is not None:<br>            # The API doesn't support empty arrays for tool_calls<br>            if not current_assistant_msg.get("tool_calls"):<br>                del current_assistant_msg["tool_calls"]<br>            result.append(current_assistant_msg)<br>            current_assistant_msg = None<br>    def ensure_assistant_message() -> ChatCompletionAssistantMessageParam:<br>        nonlocal current_assistant_msg<br>        if current_assistant_msg is None:<br>            current_assistant_msg = ChatCompletionAssistantMessageParam(role="assistant")<br>            current_assistant_msg["tool_calls"] = []<br>        return current_assistant_msg<br>    for item in items:<br>        # 1) Check easy input message<br>        if easy_msg := cls.maybe_easy_input_message(item):<br>            role = easy_msg["role"]<br>            content = easy_msg["content"]<br>            if role == "user":<br>                flush_assistant_message()<br>                msg_user: ChatCompletionUserMessageParam = {<br>                    "role": "user",<br>                    "content": cls.extract_all_content(content),<br>                }<br>                result.append(msg_user)<br>            elif role == "system":<br>                flush_assistant_message()<br>                msg_system: ChatCompletionSystemMessageParam = {<br>                    "role": "system",<br>                    "content": cls.extract_text_content(content),<br>                }<br>                result.append(msg_system)<br>            elif role == "developer":<br>                flush_assistant_message()<br>                msg_developer: ChatCompletionDeveloperMessageParam = {<br>                    "role": "developer",<br>                    "content": cls.extract_text_content(content),<br>                }<br>                result.append(msg_developer)<br>            elif role == "assistant":<br>                flush_assistant_message()<br>                msg_assistant: ChatCompletionAssistantMessageParam = {<br>                    "role": "assistant",<br>                    "content": cls.extract_text_content(content),<br>                }<br>                result.append(msg_assistant)<br>            else:<br>                raise UserError(f"Unexpected role in easy_input_message: {role}")<br>        # 2) Check input message<br>        elif in_msg := cls.maybe_input_message(item):<br>            role = in_msg["role"]<br>            content = in_msg["content"]<br>            flush_assistant_message()<br>            if role == "user":<br>                msg_user = {<br>                    "role": "user",<br>                    "content": cls.extract_all_content(content),<br>                }<br>                result.append(msg_user)<br>            elif role == "system":<br>                msg_system = {<br>                    "role": "system",<br>                    "content": cls.extract_text_content(content),<br>                }<br>                result.append(msg_system)<br>            elif role == "developer":<br>                msg_developer = {<br>                    "role": "developer",<br>                    "content": cls.extract_text_content(content),<br>                }<br>                result.append(msg_developer)<br>            else:<br>                raise UserError(f"Unexpected role in input_message: {role}")<br>        # 3) response output message => assistant<br>        elif resp_msg := cls.maybe_response_output_message(item):<br>            flush_assistant_message()<br>            new_asst = ChatCompletionAssistantMessageParam(role="assistant")<br>            contents = resp_msg["content"]<br>            text_segments = []<br>            for c in contents:<br>                if c["type"] == "output_text":<br>                    text_segments.append(c["text"])<br>                elif c["type"] == "refusal":<br>                    new_asst["refusal"] = c["refusal"]<br>                elif c["type"] == "output_audio":<br>                    # Can't handle this, b/c chat completions expects an ID which we dont have<br>                    raise UserError(<br>                        f"Only audio IDs are supported for chat completions, but got: {c}"<br>                    )<br>                else:<br>                    raise UserError(f"Unknown content type in ResponseOutputMessage: {c}")<br>            if text_segments:<br>                combined = "\n".join(text_segments)<br>                new_asst["content"] = combined<br>            new_asst["tool_calls"] = []<br>            current_assistant_msg = new_asst<br>        # 4) function/file-search calls => attach to assistant<br>        elif file_search := cls.maybe_file_search_call(item):<br>            asst = ensure_assistant_message()<br>            tool_calls = list(asst.get("tool_calls", []))<br>            new_tool_call = ChatCompletionMessageToolCallParam(<br>                id=file_search["id"],<br>                type="function",<br>                function={<br>                    "name": "file_search_call",<br>                    "arguments": json.dumps(<br>                        {<br>                            "queries": file_search.get("queries", []),<br>                            "status": file_search.get("status"),<br>                        }<br>                    ),<br>                },<br>            )<br>            tool_calls.append(new_tool_call)<br>            asst["tool_calls"] = tool_calls<br>        elif func_call := cls.maybe_function_tool_call(item):<br>            asst = ensure_assistant_message()<br>            tool_calls = list(asst.get("tool_calls", []))<br>            new_tool_call = ChatCompletionMessageToolCallParam(<br>                id=func_call["call_id"],<br>                type="function",<br>                function={<br>                    "name": func_call["name"],<br>                    "arguments": func_call["arguments"],<br>                },<br>            )<br>            tool_calls.append(new_tool_call)<br>            asst["tool_calls"] = tool_calls<br>        # 5) function call output => tool message<br>        elif func_output := cls.maybe_function_tool_call_output(item):<br>            flush_assistant_message()<br>            msg: ChatCompletionToolMessageParam = {<br>                "role": "tool",<br>                "tool_call_id": func_output["call_id"],<br>                "content": func_output["output"],<br>            }<br>            result.append(msg)<br>        # 6) item reference => handle or raise<br>        elif item_ref := cls.maybe_item_reference(item):<br>            raise UserError(<br>                f"Encountered an item_reference, which is not supported: {item_ref}"<br>            )<br>        # 7) If we haven't recognized it => fail or ignore<br>        else:<br>            raise UserError(f"Unhandled item type or structure: {item}")<br>    flush_assistant_message()<br>    return result<br>``` |